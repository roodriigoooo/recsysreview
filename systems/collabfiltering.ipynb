{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ".md explanations and justifications wip",
   "id": "c0e47b9f185891c7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-19T12:16:53.440290Z",
     "start_time": "2025-04-19T12:16:52.787692Z"
    }
   },
   "source": [
    "from utils.sim_utils import *\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "def load_data(movies_path, ratings_path):\n",
    "    movies = pd.read_csv(movies_path)\n",
    "    ratings = pd.read_csv(ratings_path)\n",
    "    return movies, ratings\n",
    "\n",
    "\n",
    "def create_rating_matrix(ratings: pd.DataFrame) -> pd.DataFrame:\n",
    "    return ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "SIMILARITY_FUNCS = {\n",
    "    'pearson': pearson,\n",
    "    'constrained_pearson': constrained_pearson,\n",
    "    'cosine': cosine,\n",
    "    'jaccard': jaccard,\n",
    "    'euclidean': euclidean,\n",
    "    'manhattan': manhattan,\n",
    "    'fast_cosine': vectorized_cosine,\n",
    "    'fast_pearson': vectorized_pearson,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T12:16:56.261337Z",
     "start_time": "2025-04-19T12:16:56.248737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_similarity_matrix(rating_matrix, method, shrinkage: Optional[float] = None):\n",
    "    if method not in SIMILARITY_FUNCS:\n",
    "        raise ValueError(f\"Invalid method: {method}\")\n",
    "    func = SIMILARITY_FUNCS.get(method)\n",
    "    if func is None:\n",
    "        raise ValueError(f\"Invalid method: {method}\")\n",
    "    if shrinkage is not None and method != 'constrained_pearson':\n",
    "        raise ValueError(f\"Shrinkage is only supported for 'constrained_pearson' method, not {method}\")\n",
    "\n",
    "    # For large matrices, use a sample for faster computation\n",
    "    sample_size = 200  # Adjust this value based on your needs\n",
    "\n",
    "    entities = rating_matrix.index\n",
    "    n = len(entities)\n",
    "\n",
    "    # If matrix is too large, use a sample\n",
    "    if n > sample_size and method != 'cosine':\n",
    "        print(f\"Using a sample of {sample_size} entities for similarity computation\")\n",
    "        # Sample entities\n",
    "        sampled_indices = np.random.choice(n, size=sample_size, replace=False)\n",
    "        sampled_entities = entities[sampled_indices]\n",
    "        rating_matrix_sample = rating_matrix.loc[sampled_entities]\n",
    "        entities = sampled_entities\n",
    "        n = len(entities)\n",
    "        data = rating_matrix_sample.values\n",
    "    else:\n",
    "        data = rating_matrix.values\n",
    "\n",
    "    # For cosine similarity, use sklearn's optimized implementation\n",
    "    if method == 'cosine':\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        # Replace NaN with 0 for cosine similarity\n",
    "        data_filled = np.nan_to_num(data)\n",
    "        sim_mat = cosine_similarity(data_filled)\n",
    "        return pd.DataFrame(sim_mat, index=entities, columns=entities)\n",
    "\n",
    "    # For other methods, use our implementation\n",
    "    sim_mat = np.zeros((n, n))\n",
    "\n",
    "    # Compute similarities\n",
    "    for i in range(n):\n",
    "        # Set diagonal to 1 (self-similarity)\n",
    "        sim_mat[i, i] = 1.0\n",
    "\n",
    "        # Only compute upper triangle (symmetric matrix)\n",
    "        for j in range(i+1, n):\n",
    "            u = data[i, :]\n",
    "            v = data[j, :]\n",
    "\n",
    "            if method == 'constrained_pearson' and shrinkage is not None:\n",
    "                score = func(u, v, shrinkage)\n",
    "            else:\n",
    "                score = func(u, v)\n",
    "\n",
    "            # Set both entries (symmetric matrix)\n",
    "            sim_mat[i, j] = score\n",
    "            sim_mat[j, i] = score\n",
    "\n",
    "    return pd.DataFrame(sim_mat, index=entities, columns=entities)\n",
    "\n",
    "def get_top_k_neighbors(sim_matrix, entity_id, k):\n",
    "    if entity_id not in sim_matrix.index:\n",
    "        return pd.Series(dtype=float)\n",
    "    if entity_id in sim_matrix.columns:\n",
    "        scores = sim_matrix.loc[entity_id].drop(index=entity_id, errors='ignore')\n",
    "        return scores.nlargest(k)\n",
    "    else:\n",
    "        return pd.Series(dtype=float)"
   ],
   "id": "2654f459f6cbadbf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T12:17:00.179044Z",
     "start_time": "2025-04-19T12:17:00.171471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_rating_user(user_id, item_id, rating_matrix, sim_matrix, k=10):\n",
    "    if user_id not in rating_matrix.index:\n",
    "        return rating_matrix.values.mean()  # Return global mean if user not found\n",
    "\n",
    "    # Check if item exists in the rating matrix\n",
    "    if item_id not in rating_matrix.columns:\n",
    "        return rating_matrix.loc[user_id].mean()  # Return user's mean if item not found\n",
    "\n",
    "    # Check if user exists in similarity matrix\n",
    "    if user_id not in sim_matrix.index:\n",
    "        return rating_matrix.loc[user_id].mean()  # Return user's mean if user not in similarity matrix\n",
    "\n",
    "    neighbors = get_top_k_neighbors(sim_matrix, user_id, k)\n",
    "    if len(neighbors) == 0:\n",
    "        return rating_matrix.loc[user_id].mean()\n",
    "\n",
    "    # Only consider neighbors who rated the item\n",
    "    neigh_ratings = rating_matrix.loc[neighbors.index, item_id]\n",
    "\n",
    "    # Create a mask for non-NaN values in neighbor ratings\n",
    "    mask = ~neigh_ratings.isna()\n",
    "    sims = neighbors[mask.values]  # Use .values to convert Series mask to array\n",
    "    ratings = neigh_ratings[mask]\n",
    "\n",
    "    # If no valid neighbors, return user's mean rating\n",
    "    if len(sims) == 0 or sims.abs().sum() == 0:\n",
    "        return rating_matrix.loc[user_id].mean()\n",
    "\n",
    "    neigh_means = rating_matrix.loc[sims.index].mean(axis=1)\n",
    "    numer = ((ratings - neigh_means) * sims).sum()\n",
    "    denom = sims.abs().sum()\n",
    "    return rating_matrix.loc[user_id].mean() + numer / denom if denom else rating_matrix.loc[user_id].mean()"
   ],
   "id": "f7127fee5916c120",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T12:17:02.439121Z",
     "start_time": "2025-04-19T12:17:02.431753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_rating_item(user_id, item_id, rating_matrix, item_sim, k = 20):\n",
    "    \"\"\"Item-based prediction with fallback to user mean.\"\"\"\n",
    "    if user_id not in rating_matrix.index:\n",
    "        return rating_matrix.values.mean()  # Return global mean if user not found\n",
    "\n",
    "    user_ratings = rating_matrix.loc[user_id].dropna()\n",
    "    if len(user_ratings) == 0:\n",
    "        return rating_matrix.values.mean()  # Return global mean if user hasn't rated anything\n",
    "\n",
    "    if item_id not in item_sim.index:\n",
    "        return user_ratings.mean()  # Return user's mean if item not in similarity matrix\n",
    "\n",
    "    # Make sure we're only getting similarities for items the user has rated\n",
    "    rated_items = list(set(user_ratings.index).intersection(set(item_sim.columns)))\n",
    "    if not rated_items:\n",
    "        return user_ratings.mean()  # Return user's mean if no overlap between rated items and similarity matrix\n",
    "\n",
    "    try:\n",
    "        sims = item_sim.loc[item_id, rated_items]\n",
    "        topk = sims.nlargest(min(k, len(sims)))\n",
    "    except KeyError:\n",
    "        # Handle case where item_id is not in similarity matrix\n",
    "        return user_ratings.mean()\n",
    "\n",
    "    if len(topk) == 0 or topk.abs().sum() == 0:\n",
    "        return user_ratings.mean()\n",
    "\n",
    "    try:\n",
    "        numer = (user_ratings[topk.index] * topk).sum()\n",
    "        denom = topk.abs().sum()\n",
    "        return numer/denom\n",
    "    except KeyError:\n",
    "        # Handle case where some indices in topk are not in user_ratings\n",
    "        return user_ratings.mean()"
   ],
   "id": "223e54f186c44c0d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T12:17:04.477548Z",
     "start_time": "2025-04-19T12:17:04.464496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def matrix_factorization(R, k=20, alpha=0.005, beta=0.02, iterations=50):\n",
    "    \"\"\"\n",
    "    Train matrix factorization via SGD:\n",
    "    Minimize sum_{(u,i) in R} (r_ui - P_u^T Q_i)^2 + beta*(||P_u||^2 + ||Q_i||^2)\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    num_users, num_items = R.shape\n",
    "    P = np.random.normal(scale=1./k, size=(num_users, k))\n",
    "    Q = np.random.normal(scale=1./k, size=(num_items, k))\n",
    "\n",
    "    # Extract known ratings\n",
    "    users, items = np.where(~np.isnan(R.values))\n",
    "    ratings = R.values[users, items]\n",
    "\n",
    "    # Use fewer iterations for faster execution\n",
    "    print(f\"Matrix factorization with {iterations} iterations...\")\n",
    "    for iter in range(iterations):\n",
    "        # Shuffle the data\n",
    "        indices = np.arange(len(ratings))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # Process in mini-batches for speed\n",
    "        batch_size = 1000\n",
    "        num_batches = len(indices) // batch_size + 1\n",
    "\n",
    "        total_error = 0\n",
    "        for batch in range(num_batches):\n",
    "            batch_indices = indices[batch*batch_size:min((batch+1)*batch_size, len(indices))]\n",
    "\n",
    "            if len(batch_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            batch_users = users[batch_indices]\n",
    "            batch_items = items[batch_indices]\n",
    "            batch_ratings = ratings[batch_indices]\n",
    "\n",
    "            for idx in range(len(batch_indices)):\n",
    "                u, i, r = batch_users[idx], batch_items[idx], batch_ratings[idx]\n",
    "                pred = P[u, :].dot(Q[i, :].T)\n",
    "                e = r - pred\n",
    "                total_error += e**2\n",
    "\n",
    "                # Update factors\n",
    "                P[u, :] += alpha * (e * Q[i, :] - beta * P[u, :])\n",
    "                Q[i, :] += alpha * (e * P[u, :] - beta * Q[i, :])\n",
    "\n",
    "        # Print progress every few iterations\n",
    "        if (iter + 1) % 5 == 0 or iter == 0:\n",
    "            rmse = np.sqrt(total_error / len(ratings))\n",
    "            print(f\"  Iteration {iter+1}/{iterations}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return P, Q\n",
    "\n",
    "def predict_rating_mf(user_id, item_id, R, P, Q):\n",
    "    \"\"\"Predict rating using matrix factorization.\"\"\"\n",
    "    # Check if user and item exist in the matrices\n",
    "    if user_id not in R.index or item_id not in R.columns:\n",
    "        # Return global mean if user or item not found\n",
    "        return R.stack().mean()\n",
    "    try:\n",
    "        # Get indices\n",
    "        user_idx = list(R.index).index(user_id)\n",
    "        item_idx = list(R.columns).index(item_id)\n",
    "        pred = P[user_idx, :].dot(Q[item_idx, :].T)\n",
    "        # Clip to rating range [0.5, 5]\n",
    "        return np.clip(pred, 0.5, 5.0)\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Error predicting for user {user_id}, item {item_id}: {e}\")\n",
    "        return R.stack().mean()"
   ],
   "id": "ce3cdca857ce69e3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T12:19:37.924448Z",
     "start_time": "2025-04-19T12:19:12.312801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rmse(preds, targets):\n",
    "    \"\"\"Compute Root Mean Squared Error ensuring arrays have the same shape.\"\"\"\n",
    "    # Check if arrays have the same length\n",
    "    if len(preds) != len(targets):\n",
    "        min_len = min(len(preds), len(targets))\n",
    "        preds = preds[:min_len]\n",
    "        targets = targets[:min_len]\n",
    "        print(f\"Warning: Arrays had different lengths. Using only the first {min_len} elements.\")\n",
    "\n",
    "    # Check if arrays are empty\n",
    "    if len(preds) == 0 or len(targets) == 0:\n",
    "        return float('nan')\n",
    "\n",
    "    return np.sqrt(np.mean((preds - targets) ** 2))\n",
    "\n",
    "def main():\n",
    "    import os\n",
    "    import time\n",
    "    from tqdm import tqdm  # for progress bars\n",
    "\n",
    "    movies_path = '../data/movies.csv'\n",
    "    ratings_path = '../data/ratings.csv'\n",
    "\n",
    "    start_time = time.time()\n",
    "    movies, ratings = load_data(movies_path, ratings_path)\n",
    "    print(f\"Data loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Take a smaller sample for faster execution\n",
    "    print(\"Taking a smaller sample for faster execution...\")\n",
    "    ratings = ratings.sample(frac=0.3, random_state=42)\n",
    "\n",
    "    # Split into train/test (leave-one-out per user)\n",
    "    print(\"Splitting data into train/test sets...\")\n",
    "    start_time = time.time()\n",
    "    test_idx = []\n",
    "    for uid, group in ratings.groupby('userId'):\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "        # randomly select one rating for test\n",
    "        sample = group.sample(n=1, random_state=42)\n",
    "        test_idx.extend(sample.index)\n",
    "    test_ratings = ratings.loc[test_idx]\n",
    "    train_ratings = ratings.drop(index=test_idx)\n",
    "    print(f\"Data split in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Train set: {len(train_ratings)} ratings, Test set: {len(test_ratings)} ratings\")\n",
    "\n",
    "    # Create rating matrix\n",
    "    print(\"Creating rating matrix...\")\n",
    "    start_time = time.time()\n",
    "    train_matrix = create_rating_matrix(train_ratings)\n",
    "    print(f\"Rating matrix created in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Matrix shape: {train_matrix.shape}\")\n",
    "\n",
    "    # User-based CF\n",
    "    print(\"\\nComputing user similarity matrix...\")\n",
    "    start_time = time.time()\n",
    "    user_sim = compute_similarity_matrix(train_matrix, method='constrained_pearson', shrinkage=10)\n",
    "    print(f\"User similarity matrix computed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    print(\"Evaluating user-based CF...\")\n",
    "    start_time = time.time()\n",
    "    y_true, y_pred_u = [], []\n",
    "\n",
    "    # Use a smaller sample for testing if the test set is large\n",
    "    eval_test = test_ratings\n",
    "    if len(test_ratings) > 1000:\n",
    "        eval_test = test_ratings.sample(n=1000, random_state=42)\n",
    "        print(f\"Using {len(eval_test)} samples for evaluation\")\n",
    "\n",
    "    try:\n",
    "        for _, row in tqdm(eval_test.iterrows(), total=len(eval_test)):\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_user(u, i, train_matrix, user_sim, k=20)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true.append(row['rating'])\n",
    "                    y_pred_u.append(pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "    except NameError:\n",
    "        for _, row in eval_test.iterrows():\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_user(u, i, train_matrix, user_sim, k=20)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true.append(row['rating'])\n",
    "                    y_pred_u.append(pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"User-based CF evaluation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"User-based CF RMSE: {rmse(np.array(y_pred_u), np.array(y_true)):.4f}\")\n",
    "\n",
    "    # Item-based CF\n",
    "    print(\"\\nComputing item similarity matrix...\")\n",
    "    start_time = time.time()\n",
    "    item_sim = compute_similarity_matrix(train_matrix.T, method='constrained_pearson', shrinkage=10)\n",
    "    print(f\"Item similarity matrix computed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    print(\"Evaluating item-based CF...\")\n",
    "    start_time = time.time()\n",
    "    y_true_i = []  # Create a new ground truth array specifically for item-based CF\n",
    "    y_pred_i = []\n",
    "\n",
    "    try:\n",
    "        for _, row in tqdm(eval_test.iterrows(), total=len(eval_test)):\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_item(u, i, train_matrix, item_sim, k=20)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true_i.append(row['rating'])  # Add the true rating\n",
    "                    y_pred_i.append(pred)  # Add the prediction\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "    except NameError:\n",
    "        # If tqdm is not available\n",
    "        for _, row in eval_test.iterrows():\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_item(u, i, train_matrix, item_sim, k=20)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true_i.append(row['rating'])  # Add the true rating\n",
    "                    y_pred_i.append(pred)  # Add the prediction\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Item-based CF evaluation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    if len(y_pred_i) > 0:\n",
    "        print(f\"Item-based CF RMSE: {rmse(np.array(y_pred_i), np.array(y_true_i)):.4f} (based on {len(y_pred_i)} predictions)\")\n",
    "    else:\n",
    "        print(\"Item-based CF: No valid predictions were made.\")\n",
    "\n",
    "    # Matrix Factorization\n",
    "    print(\"\\nTraining matrix factorization model...\")\n",
    "    start_time = time.time()\n",
    "    P, Q = matrix_factorization(train_matrix, k=20, alpha=0.005, beta=0.02, iterations=50)\n",
    "    print(f\"Matrix factorization training completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    print(\"Evaluating matrix factorization...\")\n",
    "    start_time = time.time()\n",
    "    y_true_mf = []  # Create a new ground truth array specifically for matrix factorization\n",
    "    y_pred_mf = []\n",
    "\n",
    "    try:\n",
    "        for _, row in tqdm(eval_test.iterrows(), total=len(eval_test)):\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_mf(u, i, train_matrix, P, Q)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true_mf.append(row['rating'])  # Add the true rating\n",
    "                    y_pred_mf.append(pred)  # Add the prediction\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "    except NameError:\n",
    "        for _, row in eval_test.iterrows():\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_mf(u, i, train_matrix, P, Q)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true_mf.append(row['rating'])  # Add the true rating\n",
    "                    y_pred_mf.append(pred)  # Add the prediction\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Matrix factorization evaluation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    if len(y_pred_mf) > 0:\n",
    "        print(f\"Matrix Factorization RMSE: {rmse(np.array(y_pred_mf), np.array(y_true_mf)):.4f} (based on {len(y_pred_mf)} predictions)\")\n",
    "    else:\n",
    "        print(\"Matrix Factorization: No valid predictions were made.\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    if len(y_pred_u) > 0:\n",
    "        print(f\"User-based CF RMSE: {rmse(np.array(y_pred_u), np.array(y_true)):.4f} (based on {len(y_pred_u)} predictions)\")\n",
    "    else:\n",
    "        print(\"User-based CF: No valid predictions were made.\")\n",
    "\n",
    "    if len(y_pred_i) > 0:\n",
    "        print(f\"Item-based CF RMSE: {rmse(np.array(y_pred_i), np.array(y_true_i)):.4f} (based on {len(y_pred_i)} predictions)\")\n",
    "    else:\n",
    "        print(\"Item-based CF: No valid predictions were made.\")\n",
    "\n",
    "    if len(y_pred_mf) > 0:\n",
    "        print(f\"Matrix Factorization RMSE: {rmse(np.array(y_pred_mf), np.array(y_true_mf)):.4f} (based on {len(y_pred_mf)} predictions)\")\n",
    "    else:\n",
    "        print(\"Matrix Factorization: No valid predictions were made.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "5329f655c9dce8de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 0.07 seconds\n",
      "Taking a smaller sample for faster execution...\n",
      "Splitting data into train/test sets...\n",
      "Data split in 0.28 seconds\n",
      "Train set: 29641 ratings, Test set: 610 ratings\n",
      "Creating rating matrix...\n",
      "Rating matrix created in 0.05 seconds\n",
      "Matrix shape: (610, 6096)\n",
      "\n",
      "Computing user similarity matrix...\n",
      "Using a sample of 200 entities for similarity computation\n",
      "User similarity matrix computed in 0.64 seconds\n",
      "Evaluating user-based CF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:00<00:00, 795.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF evaluation completed in 0.80 seconds\n",
      "User-based CF RMSE: 1.0571\n",
      "\n",
      "Computing item similarity matrix...\n",
      "Using a sample of 200 entities for similarity computation\n",
      "Item similarity matrix computed in 0.19 seconds\n",
      "Evaluating item-based CF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:00<00:00, 2776.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-based CF evaluation completed in 0.22 seconds\n",
      "Item-based CF RMSE: 1.0826 (based on 610 predictions)\n",
      "\n",
      "Training matrix factorization model...\n",
      "Matrix factorization with 50 iterations...\n",
      "  Iteration 1/50, RMSE: 3.6458\n",
      "  Iteration 5/50, RMSE: 3.5530\n",
      "  Iteration 10/50, RMSE: 1.9071\n",
      "  Iteration 15/50, RMSE: 1.2791\n",
      "  Iteration 20/50, RMSE: 1.0027\n",
      "  Iteration 25/50, RMSE: 0.8496\n",
      "  Iteration 30/50, RMSE: 0.7522\n",
      "  Iteration 35/50, RMSE: 0.6823\n",
      "  Iteration 40/50, RMSE: 0.6269\n",
      "  Iteration 45/50, RMSE: 0.5809\n",
      "  Iteration 50/50, RMSE: 0.5411\n",
      "Matrix factorization training completed in 21.85 seconds\n",
      "Evaluating matrix factorization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:01<00:00, 416.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix factorization evaluation completed in 1.47 seconds\n",
      "Matrix Factorization RMSE: 1.1485 (based on 610 predictions)\n",
      "\n",
      "Summary of Results:\n",
      "User-based CF RMSE: 1.0571 (based on 610 predictions)\n",
      "Item-based CF RMSE: 1.0826 (based on 610 predictions)\n",
      "Matrix Factorization RMSE: 1.1485 (based on 610 predictions)\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
