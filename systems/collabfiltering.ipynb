{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Collaborative Filtering Recommender\n",
    "\n",
    "**Collaborative filtering** is a widely used technique in recommender systems that exploits historical user-item interactions to predict preferences of users for unrated items. Collaborative filtering fall broadl into two categories:\n",
    "\n",
    "- **Neighborhood-based** (memory-based): predicts a user's rating for an item based on the ratings of similar users or items.\n",
    "- **Model-based**: builds a model from the user-item interaction data and uses this model to predict ratings (e.g., matrix factorization)."
   ],
   "id": "c0e47b9f185891c7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-19T12:16:53.440290Z",
     "start_time": "2025-04-19T12:16:52.787692Z"
    }
   },
   "source": [
    "from utils.sim_utils import *\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "def load_data(movies_path, ratings_path):\n",
    "    movies = pd.read_csv(movies_path)\n",
    "    ratings = pd.read_csv(ratings_path)\n",
    "    return movies, ratings\n",
    "\n",
    "\n",
    "def create_rating_matrix(ratings: pd.DataFrame) -> pd.DataFrame:\n",
    "    return ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "SIMILARITY_FUNCS = {\n",
    "    'pearson': pearson,\n",
    "    'constrained_pearson': constrained_pearson,\n",
    "    'cosine': cosine,\n",
    "    'jaccard': jaccard,\n",
    "    'euclidean': euclidean,\n",
    "    'manhattan': manhattan,\n",
    "    'fast_cosine': vectorized_cosine,\n",
    "    'fast_pearson': vectorized_pearson,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Neighboorhood-based collaborative filtering relies on the assumption that users who have similar preferences in the past will also have similar preferences in the future. This is based on the idea that users with similar tastes will rate items similarly. Common measures to compute pairwise similarities include:\n",
    "\n",
    "1. **Cosine similarity**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{sim}_{\\text{cos}}(u, v) = \\frac{\\sum_{i \\in I_{uv}} r_{ui} \\cdot r_{vi}}{\\sqrt{\\sum_{u \\in I_{u}} r_{ui}^2} \\sqrt{\\sum_{u \\in I_{v}} r_{vi}^2}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $I_{uv}$ is the set of items rated by both users $u$ and $v$. $r_{ui}$ and $r_{vi}$ are the ratings of user $u$ and $v$ for item $i$, respectively.\n",
    "\n",
    "> A value close to 1 indicates high similarity, while a value close to 0 indicates low similarity.\n",
    "\n",
    "2. **Pearson correlation coefficient**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{sim}_{\\text{pearson}}(u, v) = \\frac{\\sum_{i \\in I_{uv}} (r_{ui} - \\bar{r}_u) \\cdot (r_{vi} - \\bar{r}_v)}{\\sqrt{\\sum_{i \\in I_{uv}} (r_{ui} - \\bar{r}_u)^2} \\sqrt{\\sum_{i \\in I_{uv}} (r_{vi} - \\bar{r}_v)^2}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\bar{r}_u$ and $\\bar{r}_v$ are the average ratings of user $u$ and $v$, respectively. Recall that $r_{ui}$ and $r_{vi}$ are the ratings of user $u$ and $v$ for item $i$, respectively.\n",
    "\n",
    "> The formula returns a value between -1 and 1, where 1 indicates a perfect positive correlation, 0 indicates no correlation, and -1 indicates a perfect negative correlation. Note that this formula was proposed to measure linear relationships.\n",
    "\n",
    "\n",
    "\n",
    "3. **Constrained pearson correlation coefficient** (with shrinkage towards zero when few co-ratings)\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{sim}_{\\text{cpearson}}(u, v) = \\frac{n_{uv} \\cdot \\text{sim}_{\\text{pearson}}(u, v)}{n_{uv} + \\text{shrinkage}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "4. **Jaccard coefficient** for binary (thumps up/down) ratings\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{sim}_{\\text{jaccard}}(u, v) = \\frac{|I_{uv}|}{|I_u| + |I_v| - |I_{uv}|} = \\frac{|I_{uv}|}{|I_u \\cup I_v|} = \\frac{|u \\cap v|}{|u \\cup v|}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> The jaccard coefficient takes values between 0 and 1, where 1 indicates a perfect match and 0 indicates no match."
   ],
   "id": "368a5d4e7189add4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T12:16:56.261337Z",
     "start_time": "2025-04-19T12:16:56.248737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_similarity_matrix(rating_matrix, method, shrinkage: Optional[float] = None):\n",
    "    if method not in SIMILARITY_FUNCS:\n",
    "        raise ValueError(f\"Invalid method: {method}\")\n",
    "    func = SIMILARITY_FUNCS.get(method)\n",
    "    if func is None:\n",
    "        raise ValueError(f\"Invalid method: {method}\")\n",
    "    if shrinkage is not None and method != 'constrained_pearson':\n",
    "        raise ValueError(f\"Shrinkage is only supported for 'constrained_pearson' method, not {method}\")\n",
    "\n",
    "    # For large matrices, use a sample for faster computation\n",
    "    sample_size = 200\n",
    "\n",
    "    entities = rating_matrix.index\n",
    "    n = len(entities)\n",
    "\n",
    "    # If matrix is too large, use a sample\n",
    "    if n > sample_size and method != 'cosine':\n",
    "        print(f\"Using a sample of {sample_size} entities for similarity computation\")\n",
    "        # Sample entities\n",
    "        sampled_indices = np.random.choice(n, size=sample_size, replace=False)\n",
    "        sampled_entities = entities[sampled_indices]\n",
    "        rating_matrix_sample = rating_matrix.loc[sampled_entities]\n",
    "        entities = sampled_entities\n",
    "        n = len(entities)\n",
    "        data = rating_matrix_sample.values\n",
    "    else:\n",
    "        data = rating_matrix.values\n",
    "\n",
    "    # For cosine similarity, use sklearn's optimized implementation\n",
    "    if method == 'cosine':\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        data_filled = np.nan_to_num(data)\n",
    "        sim_mat = cosine_similarity(data_filled)\n",
    "        return pd.DataFrame(sim_mat, index=entities, columns=entities)\n",
    "\n",
    "    # For other methods, use our implementation\n",
    "    sim_mat = np.zeros((n, n))\n",
    "\n",
    "    # Compute similarities\n",
    "    for i in range(n):\n",
    "        # Set diagonal to 1 (self-similarity)\n",
    "        sim_mat[i, i] = 1.0\n",
    "\n",
    "        # Only compute upper triangle (symmetric matrix)\n",
    "        for j in range(i+1, n):\n",
    "            u = data[i, :]\n",
    "            v = data[j, :]\n",
    "\n",
    "            if method == 'constrained_pearson' and shrinkage is not None:\n",
    "                score = func(u, v, shrinkage)\n",
    "            else:\n",
    "                score = func(u, v)\n",
    "\n",
    "            # Set both entries (symmetric matrix)\n",
    "            sim_mat[i, j] = score\n",
    "            sim_mat[j, i] = score\n",
    "\n",
    "    return pd.DataFrame(sim_mat, index=entities, columns=entities)\n",
    "\n",
    "def get_top_k_neighbors(sim_matrix, entity_id, k):\n",
    "    if entity_id not in sim_matrix.index:\n",
    "        return pd.Series(dtype=float)\n",
    "    if entity_id in sim_matrix.columns:\n",
    "        scores = sim_matrix.loc[entity_id].drop(index=entity_id, errors='ignore')\n",
    "        return scores.nlargest(k)\n",
    "    else:\n",
    "        return pd.Series(dtype=float)"
   ],
   "id": "2654f459f6cbadbf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Sampling**: If the number of entities (users/items) exceeds `sample_size` (a threshold), sample a subset of entities to compute similarities. This is done to speed up the computation, although sacrificing some accuracy.\n",
    "- **Shrinkage**: Applied only to constrained Pearson to reduce spurious correlations when few co-ratings exist."
   ],
   "id": "948d4ecc75459733"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T12:17:00.179044Z",
     "start_time": "2025-04-19T12:17:00.171471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_rating_user(user_id, item_id, rating_matrix, sim_matrix, k=10):\n",
    "    if user_id not in rating_matrix.index:\n",
    "        return rating_matrix.values.mean()  # Return global mean if user not found\n",
    "\n",
    "    # Check if item exists in the rating matrix\n",
    "    if item_id not in rating_matrix.columns:\n",
    "        return rating_matrix.loc[user_id].mean()  # Return user's mean if item not found\n",
    "\n",
    "    # Check if user exists in similarity matrix\n",
    "    if user_id not in sim_matrix.index:\n",
    "        return rating_matrix.loc[user_id].mean()  # Return user's mean if user not in similarity matrix\n",
    "\n",
    "    neighbors = get_top_k_neighbors(sim_matrix, user_id, k)\n",
    "    if len(neighbors) == 0:\n",
    "        return rating_matrix.loc[user_id].mean()\n",
    "\n",
    "    # Only consider neighbors who rated the item\n",
    "    neigh_ratings = rating_matrix.loc[neighbors.index, item_id]\n",
    "\n",
    "    # Create a mask for non-NaN values in neighbor ratings\n",
    "    mask = ~neigh_ratings.isna()\n",
    "    sims = neighbors[mask.values]  # Use .values to convert Series mask to array\n",
    "    ratings = neigh_ratings[mask]\n",
    "\n",
    "    # If no valid neighbors, return user's mean rating\n",
    "    if len(sims) == 0 or sims.abs().sum() == 0:\n",
    "        return rating_matrix.loc[user_id].mean()\n",
    "\n",
    "    neigh_means = rating_matrix.loc[sims.index].mean(axis=1)\n",
    "    numer = ((ratings - neigh_means) * sims).sum()\n",
    "    denom = sims.abs().sum()\n",
    "    return rating_matrix.loc[user_id].mean() + numer / denom if denom else rating_matrix.loc[user_id].mean()"
   ],
   "id": "f7127fee5916c120",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T12:17:02.439121Z",
     "start_time": "2025-04-19T12:17:02.431753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_rating_item(user_id, item_id, rating_matrix, item_sim, k = 20):\n",
    "    \"\"\"Item-based prediction with fallback to user mean.\"\"\"\n",
    "    if user_id not in rating_matrix.index:\n",
    "        return rating_matrix.values.mean()  # Return global mean if user not found\n",
    "\n",
    "    user_ratings = rating_matrix.loc[user_id].dropna()\n",
    "    if len(user_ratings) == 0:\n",
    "        return rating_matrix.values.mean()  # Return global mean if user hasn't rated anything\n",
    "\n",
    "    if item_id not in item_sim.index:\n",
    "        return user_ratings.mean()  # Return user's mean if item not in similarity matrix\n",
    "\n",
    "    # Make sure we're only getting similarities for items the user has rated\n",
    "    rated_items = list(set(user_ratings.index).intersection(set(item_sim.columns)))\n",
    "    if not rated_items:\n",
    "        return user_ratings.mean()  # Return user's mean if no overlap between rated items and similarity matrix\n",
    "\n",
    "    try:\n",
    "        sims = item_sim.loc[item_id, rated_items]\n",
    "        topk = sims.nlargest(min(k, len(sims)))\n",
    "    except KeyError:\n",
    "        # Handle case where item_id is not in similarity matrix\n",
    "        return user_ratings.mean()\n",
    "\n",
    "    if len(topk) == 0 or topk.abs().sum() == 0:\n",
    "        return user_ratings.mean()\n",
    "\n",
    "    try:\n",
    "        numer = (user_ratings[topk.index] * topk).sum()\n",
    "        denom = topk.abs().sum()\n",
    "        return numer/denom\n",
    "    except KeyError:\n",
    "        # Handle case where some indices in topk are not in user_ratings\n",
    "        return user_ratings.mean()"
   ],
   "id": "223e54f186c44c0d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### User-Based Collaborative Filtering\n",
    "\n",
    "Given a target user $u$ and item $i$, we predict the rating $r_{ui}$ as follows:\n",
    "1. We find top-$K$ neighbors $N(u)$ of user $u$ based on similarity scores.\n",
    "2. We filter neighbors who rated item $i$.\n",
    "3. Compute baseline-adjusted weighted sum:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{r}_{ui} = \\bar{r}_u + \\frac{\\sum_{v \\in N(u)} \\text{sim}(u, v) \\cdot (r_{vi} - \\bar{r}_v)}{\\sum_{v \\in N(u)} |\\text{sim}(u, v)|}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\bar{r}_u$ and $\\bar{r}_v$ are the average ratings of user $u$ and $v$, respectively.\n",
    "\n",
    "> User-Based Collaborative Filtering is computationally expensive and not scalable. While it is intuitive in the way it finds like-minded users, and it is adaptive to new ratings and items, it is not suitable for large-scale applications, as it scales poorly for large numbers of users. **Sparsity** is another important issue in the user-based approach, as it is not always possible to recommend anything to users when there are very few overlapping ratings. These are best suited for small-scale applications, such as book recommendations.\n",
    "\n",
    "### Item-Based Collaborative Filtering\n",
    "\n",
    "Given a target user $u$ and item $i$, we predict the rating $r_{ui}$ as follows:\n",
    "1. Identify items $j$ that user $u$ has rated.\n",
    "2. Compute similarity scores between item $i$ and items $j$.\n",
    "3. Compute weighted sum of ratings:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{r}_{ui} = \\frac{\\sum_{j \\in I(u)} \\text{sim}(i, j) \\cdot r_{uj}}{\\sum_{j \\in I(u)} |\\text{sim}(i, j)|}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> Item-based collaborative filtering is more scalable than user-based collaborative filtering, as it does not require computing similarities between all users. However, it is not adaptive to new ratings and items, as it relies on pre-computed item similarities.\n",
    "\n",
    "> It is also more stable over time, since items generally change less than users. However, it is not suitable for new items, as it relies on pre-computed item similarities. New items can't be recommended until rated. These are best in large-scale systems with relatively stable item catalogs."
   ],
   "id": "91d508db8709c36e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T12:17:04.477548Z",
     "start_time": "2025-04-19T12:17:04.464496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def matrix_factorization(R, k=20, alpha=0.005, beta=0.02, iterations=50):\n",
    "    \"\"\"\n",
    "    Train matrix factorization via SGD:\n",
    "    Minimize sum_{(u,i) in R} (r_ui - P_u^T Q_i)^2 + beta*(||P_u||^2 + ||Q_i||^2)\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    num_users, num_items = R.shape\n",
    "    P = np.random.normal(scale=1./k, size=(num_users, k))\n",
    "    Q = np.random.normal(scale=1./k, size=(num_items, k))\n",
    "\n",
    "    # Extract known ratings\n",
    "    users, items = np.where(~np.isnan(R.values))\n",
    "    ratings = R.values[users, items]\n",
    "\n",
    "    # Use fewer iterations for faster execution\n",
    "    print(f\"Matrix factorization with {iterations} iterations...\")\n",
    "    for iter in range(iterations):\n",
    "        # Shuffle the data\n",
    "        indices = np.arange(len(ratings))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # Process in mini-batches for speed\n",
    "        batch_size = 1000\n",
    "        num_batches = len(indices) // batch_size + 1\n",
    "\n",
    "        total_error = 0\n",
    "        for batch in range(num_batches):\n",
    "            batch_indices = indices[batch*batch_size:min((batch+1)*batch_size, len(indices))]\n",
    "\n",
    "            if len(batch_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            batch_users = users[batch_indices]\n",
    "            batch_items = items[batch_indices]\n",
    "            batch_ratings = ratings[batch_indices]\n",
    "\n",
    "            for idx in range(len(batch_indices)):\n",
    "                u, i, r = batch_users[idx], batch_items[idx], batch_ratings[idx]\n",
    "                pred = P[u, :].dot(Q[i, :].T)\n",
    "                e = r - pred\n",
    "                total_error += e**2\n",
    "\n",
    "                # Update factors\n",
    "                P[u, :] += alpha * (e * Q[i, :] - beta * P[u, :])\n",
    "                Q[i, :] += alpha * (e * P[u, :] - beta * Q[i, :])\n",
    "\n",
    "        # Print progress every few iterations\n",
    "        if (iter + 1) % 5 == 0 or iter == 0:\n",
    "            rmse = np.sqrt(total_error / len(ratings))\n",
    "            print(f\"  Iteration {iter+1}/{iterations}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return P, Q\n",
    "\n",
    "def predict_rating_mf(user_id, item_id, R, P, Q):\n",
    "    \"\"\"Predict rating using matrix factorization.\"\"\"\n",
    "    # Check if user and item exist in the matrices\n",
    "    if user_id not in R.index or item_id not in R.columns:\n",
    "        # Return global mean if user or item not found\n",
    "        return R.stack().mean()\n",
    "    try:\n",
    "        # Get indices\n",
    "        user_idx = list(R.index).index(user_id)\n",
    "        item_idx = list(R.columns).index(item_id)\n",
    "        pred = P[user_idx, :].dot(Q[item_idx, :].T)\n",
    "        # Clip to rating range [0.5, 5]\n",
    "        return np.clip(pred, 0.5, 5.0)\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Error predicting for user {user_id}, item {item_id}: {e}\")\n",
    "        return R.stack().mean()"
   ],
   "id": "ce3cdca857ce69e3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Matrix Factorization\n",
    "\n",
    "Matrix factorization is a dimensionality reduction technique that decomposes a matrix into two lower-dimensional matrices. In the context of collaborative filtering, it decomposes the user-item rating matrix into two matrices: one representing user preferences and the other representing item characteristics.\n",
    "We model each user $u$ and each item $i$ as a vector in a latent space of dimension $k, p_u, q_i \\in \\mathbb{R}^{k}$. The predicted rating $r_{ui}$ is then the dot product of these two vectors:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{r}_{ui} = p_u^T q_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Our goal here is then to find two matrices $P$ and $Q$ such that $PQ^T$ approximates the original rating matrix $R$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "R \\approx PQ^T \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "by minimizing the regularized squared error\n",
    "$$\n",
    "\\begin{align*}\n",
    "min_{P, Q} \\sum_{(u, i) \\in K} (r_{ui} - p_u^T q_i)^2 + \\beta(||p_u||^2 + ||q_i||^2) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $K$ is the set of known ratings, and $\\beta$ is a regularization parameter.\n",
    "\n",
    "In this notebook, we use stochastic gradient descent (SGD) to optimize the objective function, the update rules for each rating ($u, i$) being:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_u &= p_u + \\alpha(e_{ui} \\cdot q_i - \\beta \\cdot p_u) \\\\\n",
    "q_i &= q_i + \\alpha(e_{ui} \\cdot p_u - \\beta \\cdot q_i) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $e_{ui} = r_{ui} - p_u^T q_i$ is the prediction error, $\\alpha$ is the learning rate, and $\\beta$ is the regularization parameter. Note that the total error is the sum of the errors for all ratings in the training set: $E = \\sum_{(u, i) \\in K} e_{ui}^2$.\n",
    "\n",
    "> Computing update rules:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p_u} = \\frac{\\partial}{\\partial p_u} \\sum_{(u, i) \\in K} (r_{ui} - p_u^T q_i)^2 + \\beta(||p_u||^2 + ||q_i||^2)\n",
    "$$\n",
    "> Derivative of the first term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial p_u} \\sum_{(u, i) \\in K} (r_{ui} - p_u^T q_i)^2 = -2 \\sum_{(u, i) \\in K} (r_{ui} - p_u^T q_i) q_i = -2 \\sum_{(u, i) \\in K} e_{ui} q_i\n",
    "$$\n",
    "> Derivative of the second term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial p_u} \\beta(||p_u||^2 + ||q_i||^2) = 2 \\beta p_u\n",
    "$$\n",
    "> Combining the two terms:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p_u} = -2 \\sum_{(u, i) \\in K} e_{ui} q_i + 2 \\beta p_u\n",
    "$$\n",
    "> Together:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_u &= p_u - \\gamma \\frac{\\partial L}{\\partial p_u} \\\\\n",
    "&= p_u + \\gamma(2 \\sum_{(u, i) \\in K} e_{ui} q_i - 2 \\beta p_u) \\\\\n",
    "&= p_u + 2\\gamma(\\sum_{(u, i) \\in K} e_{ui} q_i - \\beta p_u) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "> Therefore, if we repeat the same for $q_i$ (swapping $p$ and $q$), and if we update immediately one each observed pair (u, i), we get the update rule:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_u &= p_u + \\alpha(e_{ui} \\cdot q_i - \\beta \\cdot p_u) \\\\\n",
    "q_i &= q_i + \\alpha(e_{ui} \\cdot p_u - \\beta \\cdot q_i) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> where $\\alpha$ = 2$\\gamma$ is the learning rate, and $\\beta$ is the regularization parameter.\n",
    "\n",
    "Although it is more complex to implement and tune, Matrix factorization is a powerful technique that can capture complex relationships between users and items. It is adaptive to new ratings and items, as it can be updated incrementally. It captures latent tastes/features beyond the ones already observed, and therefore it can handle sparsity well. However, it is not suitable for new users, as it relies on pre-computed item similarities. New users can't be recommended until rated. These are best in large-scale systems with relatively stable item catalogs."
   ],
   "id": "c26276dfea20472c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T12:19:37.924448Z",
     "start_time": "2025-04-19T12:19:12.312801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rmse(preds, targets):\n",
    "    \"\"\"Compute Root Mean Squared Error ensuring arrays have the same shape.\"\"\"\n",
    "    # Check if arrays have the same length\n",
    "    if len(preds) != len(targets):\n",
    "        min_len = min(len(preds), len(targets))\n",
    "        preds = preds[:min_len]\n",
    "        targets = targets[:min_len]\n",
    "        print(f\"Warning: Arrays had different lengths. Using only the first {min_len} elements.\")\n",
    "\n",
    "    # Check if arrays are empty\n",
    "    if len(preds) == 0 or len(targets) == 0:\n",
    "        return float('nan')\n",
    "\n",
    "    return np.sqrt(np.mean((preds - targets) ** 2))\n",
    "\n",
    "def main():\n",
    "    import os\n",
    "    import time\n",
    "    from tqdm import tqdm  # for progress bars\n",
    "\n",
    "    movies_path = '../data/movies.csv'\n",
    "    ratings_path = '../data/ratings.csv'\n",
    "\n",
    "    start_time = time.time()\n",
    "    movies, ratings = load_data(movies_path, ratings_path)\n",
    "    print(f\"Data loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Take a smaller sample for faster execution\n",
    "    print(\"Taking a smaller sample for faster execution...\")\n",
    "    ratings = ratings.sample(frac=0.3, random_state=42)\n",
    "\n",
    "    # Split into train/test (leave-one-out per user)\n",
    "    print(\"Splitting data into train/test sets...\")\n",
    "    start_time = time.time()\n",
    "    test_idx = []\n",
    "    for uid, group in ratings.groupby('userId'):\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "        # randomly select one rating for test\n",
    "        sample = group.sample(n=1, random_state=42)\n",
    "        test_idx.extend(sample.index)\n",
    "    test_ratings = ratings.loc[test_idx]\n",
    "    train_ratings = ratings.drop(index=test_idx)\n",
    "    print(f\"Data split in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Train set: {len(train_ratings)} ratings, Test set: {len(test_ratings)} ratings\")\n",
    "\n",
    "    # Create rating matrix\n",
    "    print(\"Creating rating matrix...\")\n",
    "    start_time = time.time()\n",
    "    train_matrix = create_rating_matrix(train_ratings)\n",
    "    print(f\"Rating matrix created in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Matrix shape: {train_matrix.shape}\")\n",
    "\n",
    "    # User-based CF\n",
    "    print(\"\\nComputing user similarity matrix...\")\n",
    "    start_time = time.time()\n",
    "    user_sim = compute_similarity_matrix(train_matrix, method='constrained_pearson', shrinkage=10)\n",
    "    print(f\"User similarity matrix computed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    print(\"Evaluating user-based CF...\")\n",
    "    start_time = time.time()\n",
    "    y_true, y_pred_u = [], []\n",
    "\n",
    "    # Use a smaller sample for testing if the test set is large\n",
    "    eval_test = test_ratings\n",
    "    if len(test_ratings) > 1000:\n",
    "        eval_test = test_ratings.sample(n=1000, random_state=42)\n",
    "        print(f\"Using {len(eval_test)} samples for evaluation\")\n",
    "\n",
    "    try:\n",
    "        for _, row in tqdm(eval_test.iterrows(), total=len(eval_test)):\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_user(u, i, train_matrix, user_sim, k=20)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true.append(row['rating'])\n",
    "                    y_pred_u.append(pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "    except NameError:\n",
    "        for _, row in eval_test.iterrows():\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_user(u, i, train_matrix, user_sim, k=20)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true.append(row['rating'])\n",
    "                    y_pred_u.append(pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"User-based CF evaluation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"User-based CF RMSE: {rmse(np.array(y_pred_u), np.array(y_true)):.4f}\")\n",
    "\n",
    "    # Item-based CF\n",
    "    print(\"\\nComputing item similarity matrix...\")\n",
    "    start_time = time.time()\n",
    "    item_sim = compute_similarity_matrix(train_matrix.T, method='constrained_pearson', shrinkage=10)\n",
    "    print(f\"Item similarity matrix computed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    print(\"Evaluating item-based CF...\")\n",
    "    start_time = time.time()\n",
    "    y_true_i = []  # Create a new ground truth array specifically for item-based CF\n",
    "    y_pred_i = []\n",
    "\n",
    "    try:\n",
    "        for _, row in tqdm(eval_test.iterrows(), total=len(eval_test)):\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_item(u, i, train_matrix, item_sim, k=20)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true_i.append(row['rating'])  # Add the true rating\n",
    "                    y_pred_i.append(pred)  # Add the prediction\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "    except NameError:\n",
    "        # If tqdm is not available\n",
    "        for _, row in eval_test.iterrows():\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_item(u, i, train_matrix, item_sim, k=20)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true_i.append(row['rating'])  # Add the true rating\n",
    "                    y_pred_i.append(pred)  # Add the prediction\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Item-based CF evaluation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    if len(y_pred_i) > 0:\n",
    "        print(f\"Item-based CF RMSE: {rmse(np.array(y_pred_i), np.array(y_true_i)):.4f} (based on {len(y_pred_i)} predictions)\")\n",
    "    else:\n",
    "        print(\"Item-based CF: No valid predictions were made.\")\n",
    "\n",
    "    # Matrix Factorization\n",
    "    print(\"\\nTraining matrix factorization model...\")\n",
    "    start_time = time.time()\n",
    "    P, Q = matrix_factorization(train_matrix, k=20, alpha=0.005, beta=0.02, iterations=50)\n",
    "    print(f\"Matrix factorization training completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    print(\"Evaluating matrix factorization...\")\n",
    "    start_time = time.time()\n",
    "    y_true_mf = []  # Create a new ground truth array specifically for matrix factorization\n",
    "    y_pred_mf = []\n",
    "\n",
    "    try:\n",
    "        for _, row in tqdm(eval_test.iterrows(), total=len(eval_test)):\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_mf(u, i, train_matrix, P, Q)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true_mf.append(row['rating'])  # Add the true rating\n",
    "                    y_pred_mf.append(pred)  # Add the prediction\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "    except NameError:\n",
    "        for _, row in eval_test.iterrows():\n",
    "            try:\n",
    "                u = int(row['userId'])\n",
    "                i = int(row['movieId'])\n",
    "                pred = predict_rating_mf(u, i, train_matrix, P, Q)\n",
    "                if not np.isnan(pred):\n",
    "                    y_true_mf.append(row['rating'])  # Add the true rating\n",
    "                    y_pred_mf.append(pred)  # Add the prediction\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for user {u}, item {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Matrix factorization evaluation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    if len(y_pred_mf) > 0:\n",
    "        print(f\"Matrix Factorization RMSE: {rmse(np.array(y_pred_mf), np.array(y_true_mf)):.4f} (based on {len(y_pred_mf)} predictions)\")\n",
    "    else:\n",
    "        print(\"Matrix Factorization: No valid predictions were made.\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    if len(y_pred_u) > 0:\n",
    "        print(f\"User-based CF RMSE: {rmse(np.array(y_pred_u), np.array(y_true)):.4f} (based on {len(y_pred_u)} predictions)\")\n",
    "    else:\n",
    "        print(\"User-based CF: No valid predictions were made.\")\n",
    "\n",
    "    if len(y_pred_i) > 0:\n",
    "        print(f\"Item-based CF RMSE: {rmse(np.array(y_pred_i), np.array(y_true_i)):.4f} (based on {len(y_pred_i)} predictions)\")\n",
    "    else:\n",
    "        print(\"Item-based CF: No valid predictions were made.\")\n",
    "\n",
    "    if len(y_pred_mf) > 0:\n",
    "        print(f\"Matrix Factorization RMSE: {rmse(np.array(y_pred_mf), np.array(y_true_mf)):.4f} (based on {len(y_pred_mf)} predictions)\")\n",
    "    else:\n",
    "        print(\"Matrix Factorization: No valid predictions were made.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "5329f655c9dce8de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 0.07 seconds\n",
      "Taking a smaller sample for faster execution...\n",
      "Splitting data into train/test sets...\n",
      "Data split in 0.28 seconds\n",
      "Train set: 29641 ratings, Test set: 610 ratings\n",
      "Creating rating matrix...\n",
      "Rating matrix created in 0.05 seconds\n",
      "Matrix shape: (610, 6096)\n",
      "\n",
      "Computing user similarity matrix...\n",
      "Using a sample of 200 entities for similarity computation\n",
      "User similarity matrix computed in 0.64 seconds\n",
      "Evaluating user-based CF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:00<00:00, 795.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF evaluation completed in 0.80 seconds\n",
      "User-based CF RMSE: 1.0571\n",
      "\n",
      "Computing item similarity matrix...\n",
      "Using a sample of 200 entities for similarity computation\n",
      "Item similarity matrix computed in 0.19 seconds\n",
      "Evaluating item-based CF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:00<00:00, 2776.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-based CF evaluation completed in 0.22 seconds\n",
      "Item-based CF RMSE: 1.0826 (based on 610 predictions)\n",
      "\n",
      "Training matrix factorization model...\n",
      "Matrix factorization with 50 iterations...\n",
      "  Iteration 1/50, RMSE: 3.6458\n",
      "  Iteration 5/50, RMSE: 3.5530\n",
      "  Iteration 10/50, RMSE: 1.9071\n",
      "  Iteration 15/50, RMSE: 1.2791\n",
      "  Iteration 20/50, RMSE: 1.0027\n",
      "  Iteration 25/50, RMSE: 0.8496\n",
      "  Iteration 30/50, RMSE: 0.7522\n",
      "  Iteration 35/50, RMSE: 0.6823\n",
      "  Iteration 40/50, RMSE: 0.6269\n",
      "  Iteration 45/50, RMSE: 0.5809\n",
      "  Iteration 50/50, RMSE: 0.5411\n",
      "Matrix factorization training completed in 21.85 seconds\n",
      "Evaluating matrix factorization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:01<00:00, 416.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix factorization evaluation completed in 1.47 seconds\n",
      "Matrix Factorization RMSE: 1.1485 (based on 610 predictions)\n",
      "\n",
      "Summary of Results:\n",
      "User-based CF RMSE: 1.0571 (based on 610 predictions)\n",
      "Item-based CF RMSE: 1.0826 (based on 610 predictions)\n",
      "Matrix Factorization RMSE: 1.1485 (based on 610 predictions)\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
